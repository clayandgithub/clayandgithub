<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="关于人工智能，自然语言处理，Java WEB开发，后台架构 | 这里是 @WWW王维维 的个人博客。">
    <meta name="keyword"  content="王维维, clayoverwind, 王维维的博客, clayoverwind Blog, 博客, 个人网站, 互联网, 后端, 架构, Java，Linux, 深度学习">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>[笔记]基于python的神经网络算法实现 - 王维维的个人网站 | WWW Personal Website</title>

    <link rel="canonical" href="http://localhost:4000/2016/09/16/%E7%AC%94%E8%AE%B0-%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="http://cdn.staticfile.org/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">WWW</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                    <li>
                        <a href="/about/">About</a>
                    </li>
                    
                    <li>
                        <a href="/test/">Test</a>
                    </li>
                    
                    <li>
                        <a href="/portfolio/">Portfolio</a>
                    </li>
                    
                    <li>
                        <a href="/tags/">Tags</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/post-bg-2015.jpg" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        background-image: url('/img/post-bg-2015.jpg')
    }
</style>
<header class="intro-header" >
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/#神经网络" title="神经网络">神经网络</a>
                        
                        <a class="tag" href="/tags/#python" title="python">python</a>
                        
                    </div>
                    <h1>[笔记]基于python的神经网络算法实现</h1>
                    
                    
                    <h2 class="subheading">FROM SCRATCH</h2>
                    
                    <span class="meta">Posted by 王维维 on September 16, 2016</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

				<blockquote>
  <p>“温故而知新！”</p>
</blockquote>

<h2 id="前言">前言<span id="前言"></span></h2>

<p><a href="#正文">跳过废话，直接看正文</a></p>

<p>在实验室师兄的要求下，我在去年花了很长一段时间学习并实现了实验室师兄的一篇毕业论文（<em>胡剑青. 基于深度学习的方面级别评论情感分析[D]. 浙江大学, 2014.</em>），因此才接触到了神经网络与深度学习这个领域。</p>

<p>由于遗留下的部分代码是用matlab写的，因此我在实现这篇论文的时候，深度学习的这块代码也是用matlab来写的，由于对matlab不甚了解，用起来也不大习惯，因此遇到了不少坑，但照着<a href="http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm">UFLDL TUTORIAL</a>, 也总算完成了任务，只是大部分是在照猫画虎，对后向传播算法中的一些公式理解的还不够透彻。</p>

<p>最近在学习循环神经网络的过程中搜到了一篇文章，<a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/">IMPLEMENTING A NEURAL NETWORK FROM SCRATCH IN PYTHON – AN INTRODUCTION</a>,讲解的也非常详细，于是用python实现了下，其中同样遇到了些问题，通过查找这些问题的答案，我之前对后向传播算法的一些疑问也得到了解答。</p>

<hr />

<h2 id="正文">正文<span id="正文"></span></h2>

<h3 id="目录">目录<span id="目录"></span></h3>

<ul>
  <li><a href="#预备知识">预备知识</a></li>
  <li><a href="#代码展示">两种后向传播算法的代码展示</a></li>
  <li><a href="#算法对比">两种后向传播算法的对比</a></li>
</ul>

<h3 id="预备知识">预备知识<span id="预备知识"></span></h3>

<p>本文的目标读者是对神经网络以及后向传播算法有一定了解，但理解的不太透彻的同学，如果你还没有接触过这些知识，请<strong>依次</strong>参考以下两个入门教程，本文的主要工作与讨论也是基于这两个教程展开的。</p>

<ul>
  <li>
    <p><a href="http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm">UFLDL TUTORIAL</a></p>
  </li>
  <li>
    <p><a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/">IMPLEMENTING A NEURAL NETWORK FROM SCRATCH IN PYTHON – AN INTRODUCTION</a></p>
  </li>
</ul>

<h3 id="两种后向传播算法的代码展示">两种后向传播算法的代码展示<span id="代码展示"></span></h3>

<p>本文用python分别基于<a href="http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm">UFLDL TUTORIAL</a>以及<a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/">IMPLEMENTING A NEURAL NETWORK FROM SCRATCH IN PYTHON – AN INTRODUCTION</a>实现了两个神经网络来完成同一个任务，这两个算法都是后向传播网络，但是略有不同。通过对比这两个算法，希望你能够对神经网络有更加深刻的了解。</p>

<p>####(1) <a href="http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm">UFLDL TUTORIAL版</a>(sigmoid &amp;&amp; squared_error)</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">__author__</span> <span class="o">=</span> <span class="s">'http://clayandgithub.github.io/'</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">linear_model</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="k">class</span> <span class="nc">NNModel</span><span class="p">:</span>
    <span class="n">Ws</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># params W of the whole network</span>
    <span class="n">bs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># params b of the whole network</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># number of nodes in each layer</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c"># default learning rate for gradient descent</span>
    <span class="n">reg_lambda</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c"># default regularization strength</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">reg_lambda</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reg_lambda</span> <span class="o">=</span> <span class="n">reg_lambda</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_params</span><span class="p">()</span>

    <span class="c"># Initialize the parameters (W and b) to random values. We need to learn these.</span>
    <span class="k">def</span> <span class="nf">init_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span>
        <span class="n">hidden_layer_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">Ws</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">hidden_layer_num</span>
        <span class="n">bs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">hidden_layer_num</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">hidden_layer_num</span><span class="p">):</span>
            <span class="n">Ws</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">bs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Ws</span> <span class="o">=</span> <span class="n">Ws</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bs</span> <span class="o">=</span> <span class="n">bs</span>
    
    <span class="c"># This function learns parameters for the neural network from training dataset</span>
    <span class="c"># - num_passes: Number of passes through the training data for gradient descent</span>
    <span class="c"># - print_loss: If True, print the loss every 1000 iterations</span>
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">num_passes</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">print_loss</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">expected_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform_output_dimension</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c"># Gradient descent. For each batch...</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_passes</span><span class="p">):</span>
            
            <span class="c"># Forward propagation</span>
            <span class="n">a_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

            <span class="c"># Backpropagation</span>
            <span class="n">dWs</span><span class="p">,</span> <span class="n">dbs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">,</span> <span class="n">a_output</span><span class="p">)</span>

            <span class="c"># Update parameters of the model</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_model_params</span><span class="p">(</span><span class="n">dWs</span><span class="p">,</span> <span class="n">dbs</span><span class="p">,</span> <span class="n">num_examples</span><span class="p">)</span>

            <span class="c"># Optionally print the loss.</span>
            <span class="c"># This is expensive because it uses the whole dataset, so we don't want to do it too often.</span>
            <span class="k">if</span> <span class="n">print_loss</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s">"Loss after iteration </span><span class="si">%</span><span class="s">i: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">)))</span>

    <span class="c"># Helper function to evaluate the total loss on the dataset</span>
    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">):</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="n">expected_output</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">num_output</span> <span class="o">=</span> <span class="n">output_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="c">#training set size</span>
        <span class="n">dimension_output</span> <span class="o">=</span>  <span class="n">output_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="c"># output dimension</span>
        
        <span class="c"># Forward propagation to calculate our predictions</span>
        <span class="n">a_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">current_output</span> <span class="o">=</span> <span class="n">a_output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c"># Calculating the loss</span>
        <span class="n">data_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">current_output</span> <span class="o">-</span> <span class="n">expected_output</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c"># Add regulatization term to loss (optional)</span>
        <span class="k">for</span> <span class="n">W</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">Ws</span><span class="p">:</span>
            <span class="n">data_loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg_lambda</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">W</span><span class="p">))</span>
        <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">num_output</span> <span class="o">*</span> <span class="n">data_loss</span>

    <span class="c"># Forward propagation</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">Ws</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Ws</span>
        <span class="n">bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs</span>
        <span class="n">hidden_layer_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Ws</span><span class="p">)</span>
        <span class="n">a_output</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">hidden_layer_num</span>
        <span class="n">current_input</span> <span class="o">=</span> <span class="n">X</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">hidden_layer_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">w_current</span> <span class="o">=</span> <span class="n">Ws</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">b_current</span> <span class="o">=</span> <span class="n">bs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">z_current</span> <span class="o">=</span> <span class="n">current_input</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w_current</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_current</span>
            <span class="n">a_current</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z_current</span><span class="p">)</span>
            <span class="n">a_output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a_current</span>
            <span class="n">current_input</span> <span class="o">=</span> <span class="n">a_current</span>

        <span class="c">#output layer(logistic)</span>
        <span class="n">z_current</span> <span class="o">=</span> <span class="n">current_input</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Ws</span><span class="p">[</span><span class="n">hidden_layer_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">bs</span><span class="p">[</span><span class="n">hidden_layer_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">a_current</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z_current</span><span class="p">)</span>
        <span class="n">a_output</span><span class="p">[</span><span class="n">hidden_layer_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a_current</span>
        <span class="k">return</span> <span class="n">a_output</span>

    <span class="c"># Predict the result of classification of input x</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">a_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">a_output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c"># Backpropagation</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">,</span> <span class="n">a_output</span><span class="p">):</span>
        <span class="n">Ws</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Ws</span>
        <span class="n">bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs</span>
        <span class="n">hidden_layer_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Ws</span><span class="p">)</span>
        <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">ds</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">hidden_layer_num</span>

        <span class="c"># output layer</span>
        <span class="n">a_current</span> <span class="o">=</span> <span class="n">a_output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">d_current</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">expected_output</span> <span class="o">-</span> <span class="n">a_current</span><span class="p">)</span> <span class="o">*</span> <span class="n">a_current</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a_current</span><span class="p">)</span>
        <span class="n">ds</span><span class="p">[</span><span class="n">hidden_layer_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_current</span>

        <span class="c">#other hidden layer</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hidden_layer_num</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">w_current</span> <span class="o">=</span> <span class="n">Ws</span><span class="p">[</span><span class="n">l</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
            <span class="n">a_current</span> <span class="o">=</span> <span class="n">a_output</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
            <span class="n">d_current</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d_current</span><span class="p">,</span> <span class="n">w_current</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">a_current</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a_current</span><span class="p">)</span>
            <span class="n">ds</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_current</span>
        
        <span class="c">#calc dW &amp;&amp; db</span>
        <span class="n">dWs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">hidden_layer_num</span>
        <span class="n">dbs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">hidden_layer_num</span>
        <span class="n">a_last</span> <span class="o">=</span> <span class="n">X</span>
        <span class="n">num_output</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">hidden_layer_num</span><span class="p">):</span>
            <span class="n">d_current</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
            <span class="n">dWs</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a_last</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d_current</span><span class="p">)</span>
            <span class="n">dbs</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">d_current</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">a_last</span> <span class="o">=</span> <span class="n">a_output</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">dWs</span><span class="p">,</span> <span class="n">dbs</span>

    <span class="c"># Update the params (Ws and bs) of the netword during Backpropagation</span>
    <span class="k">def</span> <span class="nf">update_model_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dWs</span><span class="p">,</span> <span class="n">dbs</span><span class="p">,</span> <span class="n">num_examples</span><span class="p">):</span>
        <span class="n">Ws</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Ws</span>
        <span class="n">bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs</span>
        <span class="n">hidden_layer_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Ws</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">hidden_layer_num</span><span class="p">):</span>
            <span class="n">Ws</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">Ws</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">*</span> <span class="p">(</span><span class="n">dWs</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg_lambda</span> <span class="o">*</span> <span class="n">Ws</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>
            <span class="n">bs</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">bs</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">*</span> <span class="p">(</span><span class="n">dbs</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>
            <span class="c">#Ws[l] = Ws[l] - self.epsilon * (dWs[l] / num_examples + model.reg_lambda * Ws[l])</span>
            <span class="c">#bs[l] = bs[l] - self.epsilon * (dbs[l] / num_examples)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Ws</span> <span class="o">=</span> <span class="n">Ws</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bs</span> <span class="o">=</span> <span class="n">bs</span>

    <span class="c"># tranform the label matrix to output matrix (i.e. [0, 1, 1]-&gt;[[1, 0], [0, 1], [0, 1]])</span>
    <span class="k">def</span> <span class="nf">transform_output_dimension</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">class_num</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="c"># start with 0</span>
        <span class="n">examples_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">examples_num</span><span class="p">,</span> <span class="n">class_num</span><span class="p">))</span>
        <span class="n">output</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">examples_num</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">output</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">random_seed</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_seed</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.20</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">visualize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"sigmoid_squared_error_ann_classification"</span><span class="p">)</span>
    <span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">pred_func</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c"># Set min and max values and give it some padding</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="o">.</span><span class="mi">5</span>
    <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="o">.</span><span class="mi">5</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="c"># Generate a grid of points with distance h between them</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
    <span class="c"># Predict the function value for the whole gid</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">pred_func</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c"># Plot the contour and training examples</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Spectral</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Spectral</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">Config</span><span class="p">:</span>
    <span class="c"># Gradient descent parameters</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c"># learning rate for gradient descent</span>
    <span class="n">reg_lambda</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c"># regularization strength</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="c"># number of nodes in each layer</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">NNModel</span><span class="p">(</span><span class="n">Config</span><span class="o">.</span><span class="n">layers</span><span class="p">,</span> <span class="n">Config</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">Config</span><span class="o">.</span><span class="n">reg_lambda</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">print_loss</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">visualize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre>
</div>

<p>####(2) <a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/">IMPLEMENTING A NEURAL NETWORK FROM SCRATCH IN PYTHON – AN INTRODUCTION版</a>(tanh &amp;&amp; cross entropy)</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">__author__</span> <span class="o">=</span> <span class="s">'m.bashari'</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">linear_model</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="k">class</span> <span class="nc">NNModel</span><span class="p">:</span>
    <span class="n">Ws</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># params W of the whole network</span>
    <span class="n">bs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># params b of the whole network</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># number of nodes in each layer</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c"># default learning rate for gradient descent</span>
    <span class="n">reg_lambda</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c"># default regularization strength</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">reg_lambda</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reg_lambda</span> <span class="o">=</span> <span class="n">reg_lambda</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_params</span><span class="p">()</span>

    <span class="c"># Initialize the parameters (W and b) to random values. We need to learn these.</span>
    <span class="k">def</span> <span class="nf">init_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span>
        <span class="n">hidden_layer_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">Ws</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">hidden_layer_num</span>
        <span class="n">bs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">hidden_layer_num</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">hidden_layer_num</span><span class="p">):</span>
            <span class="n">Ws</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">bs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Ws</span> <span class="o">=</span> <span class="n">Ws</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bs</span> <span class="o">=</span> <span class="n">bs</span>

    <span class="c"># This function learns parameters for the neural network from training dataset</span>
    <span class="c"># - num_passes: Number of passes through the training data for gradient descent</span>
    <span class="c"># - print_loss: If True, print the loss every 1000 iterations</span>
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">num_passes</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">print_loss</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">expected_output</span> <span class="o">=</span> <span class="n">y</span>

        <span class="c"># Gradient descent. For each batch...</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_passes</span><span class="p">):</span>

            <span class="c"># Forward propagation</span>
            <span class="n">a_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

            <span class="c"># Backpropagation</span>
            <span class="n">dWs</span><span class="p">,</span> <span class="n">dbs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">,</span> <span class="n">a_output</span><span class="p">)</span>

            <span class="c"># Update parameters of the model</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_model_params</span><span class="p">(</span><span class="n">dWs</span><span class="p">,</span> <span class="n">dbs</span><span class="p">,</span> <span class="n">num_examples</span><span class="p">)</span>

            <span class="c"># Optionally print the loss.</span>
            <span class="c"># This is expensive because it uses the whole dataset, so we don't want to do it too often.</span>
            <span class="k">if</span> <span class="n">print_loss</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s">"Loss after iteration </span><span class="si">%</span><span class="s">i: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">)))</span>

    <span class="c"># Helper function to evaluate the total loss on the dataset</span>
    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">):</span>
        <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c"># training set size</span>

        <span class="c"># Forward propagation to calculate our predictions</span>
        <span class="n">a_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">a_output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c"># Calculating the loss</span>
        <span class="n">corect_logprobs</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">num_examples</span><span class="p">),</span> <span class="n">expected_output</span><span class="p">])</span>
        <span class="n">data_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">corect_logprobs</span><span class="p">)</span>
        <span class="c"># Add regulatization term to loss (optional)</span>
        <span class="k">for</span> <span class="n">W</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">Ws</span><span class="p">:</span>
            <span class="n">data_loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg_lambda</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">W</span><span class="p">))</span>
        <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">num_examples</span> <span class="o">*</span> <span class="n">data_loss</span>

    <span class="c"># Forward propagation</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">Ws</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Ws</span>
        <span class="n">bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs</span>
        <span class="n">hidden_layer_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Ws</span><span class="p">)</span>
        <span class="n">a_output</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">hidden_layer_num</span>
        <span class="n">current_input</span> <span class="o">=</span> <span class="n">X</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">hidden_layer_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">w_current</span> <span class="o">=</span> <span class="n">Ws</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">b_current</span> <span class="o">=</span> <span class="n">bs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">z_current</span> <span class="o">=</span> <span class="n">current_input</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w_current</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_current</span>
            <span class="n">a_current</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">z_current</span><span class="p">)</span>
            <span class="n">a_output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a_current</span>
            <span class="n">current_input</span> <span class="o">=</span> <span class="n">a_current</span>

        <span class="c">#output layer(softmax)</span>
        <span class="n">z_current</span> <span class="o">=</span> <span class="n">current_input</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Ws</span><span class="p">[</span><span class="n">hidden_layer_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">bs</span><span class="p">[</span><span class="n">hidden_layer_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">a_current</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">z_current</span><span class="p">)</span>
        <span class="n">a_output</span><span class="p">[</span><span class="n">hidden_layer_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a_current</span>
        <span class="k">return</span> <span class="n">a_output</span>

    <span class="c"># Predict the result of classification of input x</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">a_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">a_output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c"># Backpropagation</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">,</span> <span class="n">a_output</span><span class="p">):</span>
        <span class="n">Ws</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Ws</span>
        <span class="n">bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs</span>
        <span class="n">hidden_layer_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Ws</span><span class="p">)</span>
        <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">ds</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">hidden_layer_num</span>

        <span class="c"># output layer</span>
        <span class="n">d_current</span> <span class="o">=</span> <span class="n">a_output</span><span class="p">[</span><span class="n">hidden_layer_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">d_current</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">num_examples</span><span class="p">),</span> <span class="n">expected_output</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="n">ds</span><span class="p">[</span><span class="n">hidden_layer_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_current</span>

        <span class="c">#other hidden layer</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hidden_layer_num</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">w_current</span> <span class="o">=</span> <span class="n">Ws</span><span class="p">[</span><span class="n">l</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
            <span class="n">a_current</span> <span class="o">=</span> <span class="n">a_output</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
            <span class="n">d_current</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d_current</span><span class="p">,</span> <span class="n">w_current</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">a_current</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
            <span class="n">ds</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_current</span>
        
        <span class="c">#calc dW &amp;&amp; db</span>
        <span class="n">dWs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">hidden_layer_num</span>
        <span class="n">dbs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">hidden_layer_num</span>
        <span class="n">a_last</span> <span class="o">=</span> <span class="n">X</span>
        <span class="n">num_output</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">hidden_layer_num</span><span class="p">):</span>
            <span class="n">d_current</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
            <span class="n">dWs</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a_last</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d_current</span><span class="p">)</span>
            <span class="n">dbs</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">d_current</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">a_last</span> <span class="o">=</span> <span class="n">a_output</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">dWs</span><span class="p">,</span> <span class="n">dbs</span>

    <span class="c"># Update the params (Ws and bs) of the netword during Backpropagation</span>
    <span class="k">def</span> <span class="nf">update_model_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dWs</span><span class="p">,</span> <span class="n">dbs</span><span class="p">,</span> <span class="n">num_examples</span><span class="p">):</span>
        <span class="n">Ws</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Ws</span>
        <span class="n">bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs</span>
        <span class="n">hidden_layer_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Ws</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">hidden_layer_num</span><span class="p">):</span>
            <span class="n">Ws</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">Ws</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">*</span> <span class="p">(</span><span class="n">dWs</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg_lambda</span> <span class="o">*</span> <span class="n">Ws</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>
            <span class="n">bs</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">bs</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">*</span> <span class="p">(</span><span class="n">dbs</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>
            <span class="c">#Ws[l] = Ws[l] - self.epsilon * (dWs[l] / num_examples + model.reg_lambda * Ws[l])</span>
            <span class="c">#bs[l] = bs[l] - self.epsilon * (dbs[l] / num_examples)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Ws</span> <span class="o">=</span> <span class="n">Ws</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bs</span> <span class="o">=</span> <span class="n">bs</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">exp_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">exp_scores</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">probs</span>

<span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">random_seed</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_seed</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.20</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">visualize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"tanh_cross_entropy_ann_classification"</span><span class="p">)</span>
    <span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">pred_func</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c"># Set min and max values and give it some padding</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="o">.</span><span class="mi">5</span>
    <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="o">.</span><span class="mi">5</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="c"># Generate a grid of points with distance h between them</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
    <span class="c"># Predict the function value for the whole gid</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">pred_func</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c"># Plot the contour and training examples</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Spectral</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Spectral</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">Config</span><span class="p">:</span>
    <span class="c"># Gradient descent parameters (I picked these by hand)</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c"># learning rate for gradient descent</span>
    <span class="n">reg_lambda</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c"># regularization strength</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="c"># number of nodes in each layer</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">NNModel</span><span class="p">(</span><span class="n">Config</span><span class="o">.</span><span class="n">layers</span><span class="p">,</span> <span class="n">Config</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">Config</span><span class="o">.</span><span class="n">reg_lambda</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">print_loss</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">visualize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre>
</div>

<p>更多代码参考<a href="https://github.com/clayandgithub/nn-from-scratch">github</a></p>

<h3 id="两种后向传播算法的对比">两种后向传播算法的对比<span id="算法对比"></span></h3>

<h4 id="不同点"><strong>不同点</strong></h4>

<ul>
  <li>算法1选用了sigmoid作为激活函数，而算法2选用的是tanh作为激活函数(注：会影响输出层的delta计算)</li>
  <li>算法1选用了sigmoid作为输出层的计算(多分类版本的logistic回归)，而算法2则是选用了softmax作为输出层的计算(注：会影响输出层的结果的计算)</li>
  <li>算法1选用了squared-error作为损失函数，而算法2选用的是cross-entropy作为损失函数(注：会影响calculate_loss函数的计算以及输出层的delta计算)</li>
</ul>

<h4 id="结果对比"><strong>结果对比</strong></h4>

<p><img src="/img/sigmoid_squared_error_ann_classification.png" alt="算法1" /></p>

<p><img src="/img/tanh_cross_entropy_ann_classification.png" alt="算法2" /></p>

<p>结果大体上差不多，但是从细节上看，还是算法2效果更好一些。主要的原因应该是cross-entropy比square-error更适合用作分类任务的损失函数。</p>

<h2 id="后记">后记<span id="后记"></span></h2>

<p><strong>疑问:</strong></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c">#Ws[l] = Ws[l] - self.epsilon * (dWs[l] / num_examples + model.reg_lambda * Ws[l])</span>
<span class="c">#bs[l] = bs[l] - self.epsilon * (dbs[l] / num_examples)</span>
</code></pre>
</div>

<p>在更新W和b时，因为，dW和db是所有测试数据累计的误差，按理说应该要除以num_xamples，可是结果却无法收敛。如果不除，倒是可以收敛，不知道是因为迭代次数不够还是其他什么原因导致的。</p>

<p><strong>感悟：</strong></p>

<ul>
  <li>温故而知新。</li>
  <li>
    <p>深度学习水太深，但还是得去趟一趟，有些应用还是很有趣的。</p>

  </li>
</ul>


                <hr>

                


                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2016/09/13/%E5%B7%A5%E5%85%B7%E4%BB%A3%E7%A0%81-Swing%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%92%AD%E6%94%BE%E5%99%A8%E6%97%B6%E9%97%B4%E8%BD%B4/" data-toggle="tooltip" data-placement="top" title="[工具代码]Swing实现的播放器时间轴">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2016/09/21/%E7%AC%94%E8%AE%B0-%E5%9F%BA%E4%BA%8Etheano%E5%AE%9E%E7%8E%B0gpu%E7%89%88%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95/" data-toggle="tooltip" data-placement="top" title="[笔记]基于theano实现gpu版的神经网络算法">Next Post &rarr;</a>
                    </li>
                    
                </ul>


                
                <!-- 多说评论框 start -->
                <div class="comment">
                    <div class="ds-thread"
                        data-thread-key="/2016/09/16/[笔记]基于python的神经网络算法实现"
                        data-title="[笔记]基于python的神经网络算法实现"
                        data-url="http://localhost:4000/2016/09/16/%E7%AC%94%E8%AE%B0-%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/" >
                    </div>
                </div>
                <!-- 多说评论框 end -->
                

                

            </div>

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">
                    
                        <li><a href="#">WWW Blog</a></li>
                    
                        <li><a href="#">Foo</a></li>
                    
                        <li><a href="#">Bar</a></li>
                    
                        <li><a href="#">Example Friends</a></li>
                    
                        <li><a href="#">It helps SEO</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>


<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
    // dynamic User by Hux
    var _user = 'clayoverwind';

    // duoshuo comment query.
    var duoshuoQuery = {short_name: _user };
    (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0]
         || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
</script>
<!-- 多说公共JS代码 end -->







<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("http://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    
                    
                    <li>
                        <a href="https://twitter.com/clayoverwind">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    

                    <!-- add Weibo, Zhihu by Hux, add target = "_blank" to <a> by Hux -->
                    
                    <li>
                        <a target="_blank" href="https://www.zhihu.com/people/clayoverwind">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa  fa-stack-1x fa-inverse">知</i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li>
                        <a target="_blank" href="http://weibo.com/clayoverwind">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    


                    
                    <li>
                        <a target="_blank" href="https://www.facebook.com/clayoverwind">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li>
                        <a target="_blank" href="https://github.com/clayandgithub">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; WWW 2016
                    <br>
                    Theme by <a href="http://huangxuan.me">Hux</a> |
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="91px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=huxpro&repo=huxpro.github.io&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("http://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->

<script>
    // dynamic User by Hux
    var _gaId = 'UA-49627206-1';
    var _gaDomain = 'auto';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>



<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = '4cc1f2d8f3067386cc5cdb626a202900';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>




<!-- Image to hack wechat -->
<img src="/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
